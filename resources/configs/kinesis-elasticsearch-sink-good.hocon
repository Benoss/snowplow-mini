# Default configuration for kinesis-elasticsearch-sink

sink {

  # Sources currently supported are:
  # 'kinesis' for reading records from a Kinesis stream
  # 'stdin' for reading unencoded tab-separated events from stdin
  # If set to "stdin", JSON documents will not be sent to Elasticsearch
  # but will be written to stdout.
  source = "stdin"

  # Sinks currently supported are:
  # 'elasticsearch-kinesis' for writing good records to Elasticsearch and bad records to Kinesis
  # 'stdouterr' for writing good records to stdout and bad records to stderr
  sink = "elasticsearch-stderr"

  # The following are used to authenticate for the Amazon Kinesis sink.
  #
  # If both are set to 'default', the default provider chain is used
  # (see http://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/auth/DefaultAWSCredentialsProviderChain.html)
  #
  # If both are set to 'iam', use AWS IAM Roles to provision credentials.
  #
  # If both are set to 'env', use environment variables AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY
  aws {
    access-key: ""
    secret-key: ""
  }

  kinesis {

    in {
      stream-name: "" # Kinesis stream name

      # "good" for a stream of successfully enriched events
      # "bad" for a stream of bad events
      stream-type: "good"

      # LATEST: most recent data.
      # TRIM_HORIZON: oldest available data.
      # Note: This only affects the first run of this application
      # on a stream.
      initial-position: "TRIM_HORIZON"
    }

    out {
      # Stream for enriched events which are rejected by Elasticsearch
      stream-name: ""
      shards: 1
    }

    region: ""

    # "app-name" is used for a DynamoDB table to maintain stream state.
    # You can set it automatically using: "SnowplowElasticsearchSink-$\\{connector.kinesis.in.stream-name\\}"
    app-name: ""
  }

  elasticsearch {
    cluster-name: "elasticsearch"
    endpoint: "localhost"
    max-timeout: "10000"
    index: "good" # Elasticsearch index name
    type: "good" # Elasticsearch type name
  }

  # Events are accumulated in a buffer before being sent to Elasticsearch.
  # The buffer is emptied whenever:
  # - the combined size of the stored records exceeds byte-limit or
  # - the number of stored records exceeds record-limit or
  # - the time in milliseconds since it was last emptied exceeds time-limit
  buffer {
    byte-limit: 5242880
    record-limit: 10000
    time-limit: 60000
  }
}
